{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Restaurant Data Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_inspec = pd.read_csv(\"data/food_establishment_inspections.csv\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_311 = pd.read_csv(\"data/311_service_requests.csv\", low_memory= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get # of different types of Food complaints\n",
    "complaint_types = file_311[\"complaint_type\"].unique()\n",
    "print(complaint_types)\n",
    "calorie_counts = file_311.loc[file_311['complaint_type'] == 'Calorie Labeling']\n",
    "print(calorie_counts.shape)\n",
    "food_est = file_311.loc[file_311['complaint_type'] == 'Food Establishment']\n",
    "print(food_est.shape)\n",
    "food_poi = file_311.loc[file_311['complaint_type'] == 'Food Poisoning']\n",
    "print(food_poi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12780\n"
     ]
    }
   ],
   "source": [
    "# Count 311 complaints and food inspections for each restaurant\n",
    "def inspecKey(row):\n",
    "    return str(row['latitude']) + \",\" + str(row['longitude'])\n",
    "def estKey(row):\n",
    "    return str(row['latitude']) + \",\" + str(row['longitude'])\n",
    "\n",
    "restaurants = {} # Key = lat/long b/c unique for each restaurant\n",
    "\n",
    "# Count how many inspections each restaurant has had\n",
    "for index, row in file_inspec.iterrows():\n",
    "    key = inspecKey(row)\n",
    "    if key in restaurants:\n",
    "        restaurants[key]['inspec'] += 1\n",
    "    else:\n",
    "        restaurants[key] = { 'inspec' : 0, '311' : 0 }\n",
    "\n",
    "# Count how many 311's each restaurant has had\n",
    "overlapCount = 0\n",
    "for index, row in food_est.iterrows():\n",
    "    key = estKey(row)\n",
    "    if key in restaurants:\n",
    "        if restaurants[key]['311'] == 0:\n",
    "            overlapCount += 1\n",
    "        restaurants[key]['311'] += 1\n",
    "    else:\n",
    "        restaurants[key] = { 'inspec' : 0, '311' : 0 }\n",
    "\n",
    "print(overlapCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTA Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read demographics_city file (given by Citadel) + 4 ACS supplemental files\n",
    "file_nta_demo = pd.read_csv(\"data/demographics_city.csv\")\n",
    "supp_nta_demo = pd.read_excel(\"data/demographic_supp/nta_demo.xlsx\")\n",
    "supp_nta_econ = pd.read_excel(\"data/demographic_supp/nta_econ.xlsx\")\n",
    "supp_nta_hous = pd.read_excel(\"data/demographic_supp/nta_hous.xlsx\")\n",
    "supp_nta_soc = pd.read_excel(\"data/demographic_supp/nta_soc.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a slight under-reporting of NTAs in the Citadel demographic file\n",
    "# So get list of NTAs present in ASC survey but not in demographics_city\n",
    "for index, row in supp_nta_demo.iterrows():\n",
    "    if row['GeoID'] not in file_nta_demo['nta_code'].tolist():\n",
    "        print(row['GeoID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together all dataframes with NTA data\n",
    "nta_merged = pd.merge(file_nta_demo, pd.merge(supp_nta_soc, pd.merge(pd.merge(supp_nta_econ, supp_nta_demo, on = 'GeoID'), supp_nta_hous, on = 'GeoID')), left_on = 'nta_code', right_on = 'GeoID', how='left')\n",
    "print(nta_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine most commonly used non-English languages by New Yorkers\n",
    "langs = ['Sp', 'Fr', 'FrC', 'It', 'Prt', 'Grm', 'Yid', 'OWG', 'Scd', 'Grk', 'Rus', 'Pol', 'SCr', 'OSv', 'Arm', 'Prs', 'Guj', 'Hdi', 'Urd', 'OIn', 'OIE', 'Chi', 'Jap', 'Kor', 'MKm', 'Hmg', 'Tha', 'Lao', 'Vie', 'OAn', 'Tag', 'OPI', 'Nav', 'ONA', 'Hng', 'Arb', 'Heb', 'Afr',]\n",
    "soc_langs = []\n",
    "for l in langs:\n",
    "    soc_langs.append({ 'lang' : l, 'val' : supp_nta_soc['Lg'+l+'LEP2E'].sum()})\n",
    "\n",
    "print(sorted(soc_langs, key= lambda x : x['val'], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store interesting NTA demographic data in dictionary\n",
    "nta_demographics = {}\n",
    "nta_demographics_dataframe_prototype = []\n",
    "for index, row in nta_merged.iterrows():\n",
    "    nta_demographics[row['nta_code']] = {\n",
    "        'pop' : row['population'],\n",
    "        'households' : row['households'],\n",
    "        'ages' : [ \n",
    "            row['under_5_years'], row['5-9_years'], row['10-14_years'], row['15-19_years'], row['20-24_years'], \n",
    "            row['25-29_years'], row['30-34_years'], row['35-39_years'], row['40-44_years'], row['45-49_years'], \n",
    "            row['50-54_years'], row['55-59_years'], row['60-64_years'], row['over_65_years'],\n",
    "        ],\n",
    "        'median_hh_income' : row['median_income'],\n",
    "        'hh_incomes' : [ # Number of house holds in given income bracket\n",
    "            row['less_than_10,000'], row['10000_to_14999'], row['15000_to_24999'], row['25000_to_34999'], \n",
    "            row['35000_to_49999'], row['50000_to_74999'], row['75000_to_99999'], row['100000_to_149999'], \n",
    "            row['150000_to_199999'], row['200000_or_more'],\n",
    "        ],\n",
    "        'below_poverty' : row['PBwPvE'], # Number of people living below poverty line\n",
    "        'education_level' : [ # 25+ years, highest level of education attained\n",
    "            int(row['EA_LTHSGrE']), # No high school\n",
    "            int(row['EA_HScGrdE']), # High school grad\n",
    "            int(row['EA_AscDE'])+int(row['EA_BchDE']) + int(row['EA_GrdPfDE']), # College/graduate degree\n",
    "        ],\n",
    "        'commute' : { # How ppl commute to work\n",
    "            'walk' : row['CW_WlkdE'],\n",
    "            'public' : row['CW_PbTrnsE'],\n",
    "            'car' : int(row['CW_CrpldE'])+int(row['CW_DrvAlnE']),\n",
    "        },\n",
    "        'health_insurance_coverage' : row['HInsE'], # Number of ppl with public or private health insurance\n",
    "        'disability' : row['CvNIDE'], # Number of ppl living with disability\n",
    "        'not_us_citizen' : row['FbNotCznE'], # Number of non-US citizens\n",
    "        'languages' : { # Number of ppl who speak each language and do not speak English \"very well\" - Note: These are the top 6 such languages in NYC\n",
    "            'spanish' : row['LgSpLEP2E'],\n",
    "            'chinese' : row['LgChiLEP2E'],\n",
    "            'russian' : row['LgRusLEP2E'],\n",
    "            'indic' : row['LgOInLEP2E'],\n",
    "            'french_creole' : row['LgFrCLEP2E'],\n",
    "            'korean' : row['LgKorLEP2E'],\n",
    "        },\n",
    "        'home_value' : [\n",
    "            row['VlU50E'], # <50k\n",
    "            row['Vl50t99E'], # 50-99k\n",
    "            row['Vl100t149E'], # 100-149k\n",
    "            row['Vl150t199E'], # 150-199k\n",
    "            row['Vl200t299E'], # 200-299k\n",
    "            row['Vl300t499E'], # 300-499k\n",
    "            row['Vl500t999E'], # 500-999k\n",
    "            row['Vl1milplE'], # 1M + \n",
    "        ],\n",
    "        'rent' : [\n",
    "            row['GRU500E'], # <500\n",
    "            row['GR500t999E'], # 500-999\n",
    "            row['GR1kt14kE'], # 1-1.5k\n",
    "            row['GR15kt19kE'], # 1.5-2k\n",
    "            row['GR20kt24kE'], # 2-2.5k\n",
    "            row['GR25kt29kE'], # 2.5-3k\n",
    "            row['GR3kplE'], # 3k+\n",
    "        ],\n",
    "        'average_hh_size' : row['AvgHHSzE'],\n",
    "    }\n",
    "    nta_demographics_dataframe_prototype.append([\n",
    "        row['nta_code'],\n",
    "        row['population'], row['households'],\n",
    "        row['under_5_years'], row['5-9_years'], row['10-14_years'], row['15-19_years'], row['20-24_years'], row['25-29_years'], row['30-34_years'], row['35-39_years'], row['40-44_years'], row['45-49_years'], row['50-54_years'], row['55-59_years'], row['60-64_years'], row['over_65_years'],\n",
    "        row['median_income'],\n",
    "        row['less_than_10,000'], row['10000_to_14999'], row['15000_to_24999'], row['25000_to_34999'], row['35000_to_49999'], row['50000_to_74999'], row['75000_to_99999'], row['100000_to_149999'], row['150000_to_199999'], row['200000_or_more'],\n",
    "        row['PBwPvE'],\n",
    "        row['EA_LTHSGrE'], row['EA_HScGrdE'], int(row['EA_AscDE'])+int(row['EA_BchDE']) + int(row['EA_GrdPfDE']),\n",
    "        row['CW_WlkdE'], row['CW_PbTrnsE'], int(row['CW_CrpldE'])+int(row['CW_DrvAlnE']),\n",
    "        row['HInsE'],\n",
    "        row['CvNIDE'],\n",
    "        row['FbNotCznE'],\n",
    "        row['LgSpLEP2E'], row['LgChiLEP2E'], row['LgRusLEP2E'], row['LgOInLEP2E'], row['LgFrCLEP2E'], row['LgKorLEP2E'],\n",
    "        row['VlU50E'], row['Vl50t99E'], row['Vl100t149E'], row['Vl150t199E'], row['Vl200t299E'], row['Vl300t499E'], row['Vl500t999E'], row['Vl1milplE'],\n",
    "        row['GRU500E'], row['GR500t999E'], row['GR1kt14kE'], row['GR15kt19kE'], row['GR20kt24kE'], row['GR25kt29kE'], row['GR3kplE'], row['AvgHHSzE'],\n",
    "    ])\n",
    "\n",
    "print(nta_demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTA dictionary -> Pandas Dataframe\n",
    "nta_dataframe = pd.DataFrame(nta_demographics_dataframe_prototype,\n",
    "                                      columns = ['nta_code', 'pop', 'households', 'age_less_5', 'age_5_to_9', 'age_10_to_14', 'age_15_to_19', 'age_20_to_24', 'age_25_to_29', 'age_30_to_34', 'age_35_to_39', 'age_40_to_44', 'age_45_to_49', 'age_50_to_54', 'age_55_to_59', 'age_60_to_64', 'age_65_plus', 'median_hh_income', 'hh_income_less_10', 'hh_income_10_to_15', 'hh_income_15_to_25', 'hh_income_25_to_35', 'hh_income_35_to_50', 'hh_income_50_to_75', 'hh_income_75_to_100', 'hh_income_100_to_150', 'hh_income_150_to_200', 'hh_income_200_plus', 'below_poverty', 'edu_none', 'edu_high_school', 'edu_college', 'commute_walk', 'commute_public', 'commute_car', 'health_insurance_coverage', 'disability', 'not_us_citizen', 'lang_spanish', 'lang_chinese', 'lang_russian', 'lang_indic', 'lang_french_creole', 'lang_korean', 'homeval_less_50', 'homeval_50_to_100', 'homeval_100_to_150', 'homeval_150_to_200', 'homeval_200_to_300', 'homeval_300_to_500', 'homeval_500_to_1m', 'homeval_1m_plus', 'rent_less_500', 'rent_500_to_1000', 'rent_1000_to_1500', 'rent_1500_to_2000', 'rent_2000_to_2500', 'rent_2500_to_3000', 'rent_3000_plus', 'average_hh_size', ])\n",
    "print(nta_dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# County Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Citadel's demographic file + \n",
    "file_county_demo = pd.read_csv(\"data/demographics_state.csv\")\n",
    "# supp_county_demo = pd.read_excel(\"data/demographic_supp/county_demo.xlsx\")\n",
    "# supp_county_econ = pd.read_excel(\"data/demographic_supp/county_econ.xlsx\")\n",
    "# supp_county_hous = pd.read_excel(\"data/demographic_supp/county_hous.xlsx\")\n",
    "# supp_county_soc = pd.read_excel(\"data/demographic_supp/county_soc.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citadel reports years 2011-16\n",
    "# Filter file to only take data from the year 2016 (most recent data)\n",
    "file_county_demo = file_county_demo.loc[file_county_demo[\"year\"] == 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together all dataframes with county data\n",
    "county_merged = file_county_demo\n",
    "# pd.merge(file_county_demo, pd.merge(supp_nta_soc, pd.merge(pd.merge(supp_nta_econ, supp_nta_demo, on = 'GeoID'), supp_nta_hous, on = 'GeoID')), left_on = 'nta_code', right_on = 'GeoID', how='left')\n",
    "# print(county_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store interesting county demographic data in dictionary\n",
    "county_demographics = {}\n",
    "county_demographics_pandas_prototype = []\n",
    "for index, row in county_merged.iterrows():\n",
    "    county = row['geography']\n",
    "    if county == \"New York\": \n",
    "        continue # Skip state info\n",
    "    else: \n",
    "        county = county.replace(\", New York\", \"\") # Remove \", New York\" from county name\n",
    "    county_demographics[county] = {\n",
    "        'pop': row['population'],\n",
    "        'households' : row['total_households'],\n",
    "        'hh_incomes' : {\n",
    "            row['$9,999_or_less'], row['$10,000_to_$14,999'], row['$15,000_to_$24,999'], \n",
    "            row['$25,000_to_$34,999'], row['$35,000_to_$49,999'], row['$50,000_to_$74,999'],\n",
    "            row['$75,000_to_$99,999'], row['$100,000_to_$150,000'], row['$150,000_to_$199,999'], \n",
    "            row['$200,000_or_more'],\n",
    "        },\n",
    "        'median_hh_income' : row['median_household_income'],\n",
    "        'mean_soc_sec_income' : row['mean_soc_sec'], # Mean Social Security income\n",
    "        'food_stamps' : row['food_stamp_benefits'], # Total SNAP benefits\n",
    "        'health_insurance_coverage' : row['pop_w_health_insurance'],\n",
    "    \n",
    "    \n",
    "    }\n",
    "    county_demographics_pandas_prototype.append([\n",
    "        county,\n",
    "        row['population'],\n",
    "        row['total_households'],\n",
    "        row['$9,999_or_less'], row['$10,000_to_$14,999'], row['$15,000_to_$24,999'], \n",
    "        row['$25,000_to_$34,999'], row['$35,000_to_$49,999'], row['$50,000_to_$74,999'],\n",
    "        row['$75,000_to_$99,999'], row['$100,000_to_$150,000'], row['$150,000_to_$199,999'], \n",
    "        row['$200,000_or_more'],\n",
    "        row['median_household_income'],\n",
    "        row['mean_soc_sec'],\n",
    "        row['food_stamp_benefits'],\n",
    "        row['pop_w_health_insurance'],\n",
    "    ])\n",
    "\n",
    "print(county_demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# County dictionary -> Pandas Dataframe\n",
    "county_dataframe = pd.DataFrame(county_demographics_pandas_prototype,\n",
    "                                      columns = ['county_name', 'population', 'total_households', 'hh_income_less_10', 'hh_income_10_to_15', 'hh_income_15_to_25', 'hh_income_25_to_35', 'hh_income_35_to_50', 'hh_income_50_to_75', 'hh_income_75_to_100', 'hh_income_100_to_150', 'hh_income_150_to_200', 'hh_income_200_plus', 'median_household_income', 'mean_soc_sec', 'food_stamp_benefits', 'pop_w_health_insurance',])\n",
    "print(county_dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brainstorming\n",
    "\n",
    "Variable (file:column) POSITIVE/NEGATIVE Correlation - Rationale\n",
    "\n",
    "- Demographics\n",
    "    - Number of people (demographics_city:population) NEGATIVE - more people needing service means higher prioritization, 311 system more used to deploying in these areas\n",
    "    - Population density (demographics_city:people_per_acre), NEGATIVE - more people means higher prioritization, denser areas in city are wealthier, more bang for buck in terms of resource allocation when servicing denser rather than sparse areas \n",
    "    - Median age (demographics_city:median_age)\n",
    "    - Median income (demographics_city:median_income)\n",
    "    - 5-yr age brackets (demographics_city:age_brackets)\n",
    "    - Distribution of HH income (demographics_city:income_range), NEGATIVE - higher incomes get more gov service\n",
    "\n",
    "\n",
    "Cleaning the supp files:\n",
    "    - Note that there were 7 NTAs present in the ACS 2016 dataset for NYC that weren't present in the demographics_city.csv file. Looking at a map of NYC's NTAs, the blocks were: BX99 (\"parks, cemeteries, etc.\" in the Bronx), BX98 (Riker's Island), MN99 (\"parks, cemeteries, etc.\" in Manhattan), QN98 (\"airports\"), QN99 (\"parks, cemeteries, etc.\" in Queens), and SI99 (\"parks, cemeteries, etc.\" in Staten Island)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
